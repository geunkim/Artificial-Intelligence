# 문장과 토큰(Sentence and Token)

문장은 일련의 토큰(token)으로 구성되며 텍스트 토큰은 주관적, 임의적인 성격을 갖는다. 
문장에서 토큰을 나누는 기준은 공백(white space), 형태소(morphs), 어절, 비트숫자 등 다양하다.

## 단어장(Vocaburary)
자연어 처리를 위해 고려하는 단어들의 집합으로 단순한 단어의 변형 형태(예: book, books, dog, dogs)도 다른 단어로 인식한다.
텍스트에 서로 다른 단어가 총 1,000개가 존재하면 단어장의 크기는 1,000 이다. 
1,000개의 단어는 인코딩(encoding)과정에서 1번부터 1,000번까지 인덱스를 부여한다. 

컴퓨터는 문자보다는 숫자를 더 잘 처리한다. 그러므로 자연어 처리에서도 문자를 숫자로 바꾸는 기법을 사용하고 있다. 
단어장의 단어들은 중복되지 않는 인덱스(index)를 부여하는 것을 인코딩(Encoding)이라 한다. 
즉 단어들을 컴퓨터가 이해할 수 있는 숫자로 변경하는 인코딩과정이 필요하다. 
  
## 원-핫 인코딩(One Hot Encoding)

단어장을 구성하는 단어의 크기가 ```N``` 개일 때 ```N``` 차원의 벡터를 정의하고 
각 단어를 ```N``` 차원의 벡터에서 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고 다른 인덱스에는 0을 부여하는 
형식으로 관련 단어를 벡터 형식으로 표현하는 것이다. 단어의 인덱스 위치에 있는 값은 1, 나머지는 0으로 구성한다.

  ![equation](http://latex.codecogs.com/gif.latex?x=[0,0,..,0,1,0,..,0.0]\in\{0,1}|V|)
  
* 한계: 단어장의 크기가 커질 수록 벡터의 차수가 지속적으로 증가하게 된다. 
각 단어가 별도의 축을 가지므로 단어의 유사도를 표현하지 못하는 단점이 있다. 
단어간 유사성을 알 수 없다는 점은 검색 시스템 등에서 심각한 문제를 야기한다. 

이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화하는 기법으로 크게 두 가지가 있다. 
첫쨰는 카운트 기반 벡터화 방법으로 LSA, HAL 등이 있으며 둘쨰는 예측기반 벡터화 방법으로 NNLM, RNNLM, Word2Vec, FastText 등이 있다.
카운트 기반과 예측 기반 방법 모두를 사용하는 방법을 GloVe 라는 방법이 있다. 

## 임베딩(Embeding)

각 토큰을 연속 벡터 공간(Continous vector space)에 투영하는 것으로 각 원-핫 인코딩된 벡터(```x```)와
연속 벡터 공간(```W```)을 내적하는 **Table Lookup Up**과정을 거쳐 문장의 토콘은 연속적이이고 높은 차원의
벡터로 변환한다.

* CBoW(Continous bag-of-words)
  - 단어장을 순서가 없는 단어 주머니로 생각하고 문장에 대한 표현을 단어 벡터들을 평균시킨 벡터로 한다.
  - 의미가 비슷한 단어들은 공간 상에서 가깝게 위치하고 의미가 비슷하지 않으면 공간 상에서 멀리 떨어져 있도록 한다. 

* Relation Network(Skip-Bigram)
  - 문장 내의 모든 토큰 쌍에 대해서 신경망을 만들어서 문장 표현을 찾는 것으로 여러 단어로 된 표현을 탐지할 수 있으나 모든 단어 간의 관계를 보기 때문에
  전혀 연관이 없는 단어도 보게 되어 계산 복잡도가 높아진다.
  
* Convolution Neural Network (CNN)
  - **n-gram**을 단어, 다중 단어 표현, 구절, 문장 순으로  
  - 계

