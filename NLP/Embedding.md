# 임베딩(Embedding)





각 토큰을 연속 벡터 공간(Continous vector space)에 투영하는 것으로 각 원-핫 인코딩된 벡터(```x```)와
연속 벡터 공간(```W```)을 내적하는 **Table Lookup Up**과정을 거쳐 문장의 토콘은 연속적이이고 높은 차원의
벡터로 변환한다.

* CBoW(Continous bag-of-words)
  - 단어장을 순서가 없는 단어 주머니로 생각하고 문장에 대한 표현을 단어 벡터들을 평균시킨 벡터로 한다.
  - 의미가 비슷한 단어들은 공간 상에서 가깝게 위치하고 의미가 비슷하지 않으면 공간 상에서 멀리 떨어져 있도록 한다. 

* Relation Network(Skip-Bigram)
  - 문장 내의 모든 토큰 쌍에 대해서 신경망을 만들어서 문장 표현을 찾는 것으로 여러 단어로 된 표현을 탐지할 수 있으나 모든 단어 간의 관계를 보기 때문에
  전혀 연관이 없는 단어도 보게 되어 계산 복잡도가 높아진다.
  
* Convolution Neural Network (CNN)
  - **n-gram**을 단어, 다중 단어 표현, 구절, 문장 순으로  